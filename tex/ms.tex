\documentclass[a4paper, 12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage[left=1.5cm, right=1.5cm, bottom=2cm, top=2cm]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{natbib}
\newcommand{\logsumexp}{\textnormal{logsumexp}}
\newcommand{\logdiffexp}{\textnormal{logdiffexp}}
\newcommand{\sign}{\textnormal{sign}}

\title{The LBRY Trending Algorithm}
\author{Brendon J. Brewer}
\date{}

\begin{document}
\maketitle

\abstract{I describe the most recent update to the LBRY trending
algorithm. The ideas are similar to previous implementations, but with a
few new features.}

% Need this after the abstract
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

\section{Introduction}
LBRY \citep{li2020lbry} needs a trending algorithm to rank claims by `recent popularity',
preferably using on-chain information such as changes in support. An
intuitive idea is to have increases and decreases in overall support cause
a `spike' in the trending score, which then fades away over time
(assuming no further changes occur). This idea has formed the basis of all the
implementations that I have written.

All claims, be they streams/files, channels, reposts, or collections, have
an associated trending score. Hubs (formerly wallet servers) compute and
save the trending score of every claim along with the other data. Several
views of content on LBRY Desktop and Odysee involve sorting claims by their
trending score, in descending order.

In 2021, Jack Robison rewrote a lot of the hub code to improve performance
and to ensure hub data integrity when a chain reorganisation occurs. The
backend now uses leveldb to store transaction and claim information, and
Elasticsearch to store claim information in a suitable form for clients to
query. Leveldb will be replaced with rocksdb soon.
As part of these changes, trending needed to be made more efficient. This
meant we made some modifications to it, some of which are pretty neat
in my opinion.

\section{Exponentially Decaying Spikes}
Suppose that, at time (block height) $t$, the total amount of LBC staked
towards a claim is $\ell_t$, and its trending score is $Y_t$. At height
$(t+1)$, if the total amount of LBC remains the same, the trending score
is decayed towards zero:
\begin{align}
Y_{t+1} &= kY_t, \label{eqn:decay}
\end{align}
where $k$ is a decay constant which we set equal to
$\exp(-1/576) \approx 0.9982653$, so that the e-folding timescale is 576
blocks, approximately 24 hours.

If an LBC change occurs, the update is instead
\begin{align}
Y_{t+1} &= kY_t + M(\ell_t, \ell_{t+1}), \label{eqn:decay_with_spike}
\end{align}
where the second term is the `mass' of a `trending spike', and depends on
the old and new LBC values. A simple idea is to set the mass of the trending
spike equal to the change in LBC:
\begin{align}
M(\ell_t, \ell_{t+1}) &= \ell_{t+1} - \ell_t.
\end{align}
For a claim created at height $t=50$ with 1 LBC staked towards it, and then
later supported with 3 more LBC at height $t=550$, the trending score
would respond as in Figure~\ref{fig:simple}.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.7\textwidth]{figures/simple.pdf}
\caption{An example of how the trending score might vary over time
for a single claim.\label{fig:simple}}
\end{figure}

However, in practice this simple choice might lead to undesirable results.
Many supports on the LBRY network are created by publishers themselves,
attempting to boost their own content's visibility. If an item receives
many organic views and a moderate number of small tips, one might expect it
to trend, yet with the linear choice for $M$, it will have no hope of
competing with a `whale' who can deposit 100,000 LBC in support without
blinking an eye. In Section~\ref{sec:spike_mass}, we simulate several
different patterns of support, and choose a function $M$ such that
the size of trending spikes makes `minnows' able to compete with `whales',
at least to a greater extent than they can with the linear choice.

\section{Nonlinearity --- the Spike Mass Function}\label{sec:spike_mass}
As briefly discussed in the last section, a linear choice of spike mass
function might not be ideal. When we choose this function, we are making
a decision about how significant one amount of LBC at one time is, compared
to a different amount of LBC at a different time. For example, perhaps
1000 LBC right now should be considered as
significant as 10,000 LBC 12 hours ago.

\subsection{Change in Softened LBC Amount}
Our final choice of spike mass function is complex. I will describe simpler
versions at first, which will help in motivating the final, complicated, choice.
First, let's look at a choice where the LBC amounts are `softened' by raising
them to some power less than one (here I have chosen a power of 1/3, in
previous versions of the algorithm I used 1/4):
\begin{align}
M_1(\ell_t, \ell_{t+1}) &= \ell_{t+1}^{1/3} - \ell_t^{1/3}.
\end{align}
The size of a trending spike here would be the change in the `softened'
LBC amount. A change from 0 to 1 LBC would give a spike mass of 1,
as would a change from 1 to 1000 LBC, or 1000 to 1,000,000. It takes more
LBC to move a mountain than to move a molehill.

With this choice, it wouldn't matter whether a change took place in one large
step or in smaller increments\footnote{This would occur with any
choice of the form $M(x, y) = \Phi(y) - \Phi(x)$ for a monotonic function
$\Phi()$.}. Specifically,
\begin{align}
M_1(x, y) + M_1(y, z)
    &= y^{1/3} - x^{1/3} + z^{1/3} - y^{1/3} \\
    &= z^{1/3} - x^{1/3} \\
    &= M_1(x, z).
\end{align}
This may not be entirely desirable. Genuinely popular claims on LBRY tend
to receive several smaller supports, rather than one large one, and intuitively
this ought to help. A trending algorithm should show what's trending,
as well as what's recently been boosted.


\subsection{Softened Change in LBC Amount}
Instead of applying the power before the subtraction, we could apply the
subtraction first --- i.e., we could use
\begin{align}
M_2(\ell_t, \ell_{t+1}) &= \left(\ell_{t+1} - \ell_t\right)^{1/3}.
\end{align}
The subscript 2 distinguishes this choice from $M_1()$ in the previous
subsection.
This would make several small supports more effective than one large support,
in terms of the trending score, a potentially desirable feature:
\begin{align}
M_2(x, y) + M_2(y, z)
    &= (y - x)^{1/3} + (z - y)^{1/3} \\
    &> (z - x)^{1/3}.
\end{align}
The inequality follows from (I don't know --- Jensen's inequality perhaps).
However, in numerical experiments, we found that this effect was too strong,
and whales could attain huge heights in trending score by splitting up, say,
100,000 LBC, into several supports.

\subsection{Interpolation}
The two ideas $M_1()$ and $M_2()$ can be merged into a compromise third option,
as follows:
\begin{align}
M_3(\ell_t, \ell_{t+1}) &= M_1(\ell_t, \ell_{t+1})^\alpha
                           M_2(\ell_t, \ell_{t+1})^{1 - \alpha}. \label{eqn:spike_mass}
\end{align}
This is linear interpolation of $\log M$ values, or alternatively, it is
`geometric interpolation'. If $\alpha$ is close to 1 then the rule is more
like $M_1$, if it is close to zero it is more like $M_2$.

What we actually settled on was to have $\alpha \approx 0.5$ when we're dealing
with low LBC values, and $\alpha = 0.85$ when dealing with higher LBC values.
There is a narrow range over which $\alpha$ is between these two values:
\begin{align}
\alpha &= \left\{
        \begin{array}{lr}
            0.5, & \ell_2 \leq 50,\\
            0.01\ell_2, & 50 < \ell_2 \leq 85,\\
            0.85, & \textnormal{otherwise}.
        \end{array}
        \right.
\end{align}
This completes the specification of the spike mass function.

% TODO: Consider idea of symmetrising by using both old and new LBC values,
% as this may make reorg safety occur naturally without any extra effort


\section{Sparsity}
For most claims, most of the time, nothing happens. However, computing the
exponential decay using Equation~\ref{eqn:decay} would still incur a cost.
Right now, there are about 15 million active claims on the LBRY network,
and the intention is that there might be orders of magnitude more in the
future. We want to avoid decaying the trending score of 15 million claims
when nothing has happened to most of them. In the most recent implementation,
we have devised a way to avoid decaying any claims, ever --- only touched claims
(those whose LBC values changed) ever need any computation.

The formula for updating a trending score is Equation~\ref{eqn:decay_with_spike}:
\begin{align}
Y_{t+1} &= kY_t + M(\ell_t, \ell_{t+1}).
\end{align}
However, instead of storing $Y$ for each claim, we could instead store
a transformed version of it:
\begin{align}
Z_t &= Y_t k^{-t}.
\end{align}
The updating formula for $Y$ can then be rewritten as
\begin{align}
Z_{t+1} k^{t+1} &= kZ_t k^t + M(\ell_t, \ell_{t+1}).
\end{align}
Solving for $Z_{t+1}$ gives
\begin{align}
Z_{t+1} &= kZ_t k^t k^{-(t+1)} + M(\ell_t, \ell_{t+1})k^{-(t+1)} \\
        &= Z_t + M(\ell_t, \ell_{t+1})k^{-(t+1)}. \label{eqn:sparse_update}
\end{align}
Thus, if we maintain $Z$ instead of $Y$, 
we replace `exponential decay' with maintenance of a constant trending
score for all untouched claims --- the formula reduces to $Z_{t+1}=Z_t$.
As time goes on, the mass of applied spikes
merely grows exponentially (recall that $k$ is just below 1, so $k$
raised to a negative power becomes large and positive),
and that is how recent changes become more significant than old ones.

The only difficulty now is how to add exponentially-large values to touched
claims while avoiding overflow. This requires another nonlinear transformation
which I informally call `squashing'.

\section{The `Squashed' Grade}
Suppose $x$ is a huge number, where double precision overflow might occur if
we were to use it in a calculation (this happens at around $10^{308}$).
If $x$ is positive, we could instead deal with logarithms. Then, to add
two huge numbers without overflow, we could use
\begin{align}
\log(x + y)  &= \log(\exp(\log x) + \exp(\log y)) \\
             &= \logsumexp(\log x, \log y),
\end{align}
where $\logsumexp()$ is given by
\begin{align}
\logsumexp(a, b) &= \max(a, b)
        + \log\left[\exp(a - \max(a, b)) + \exp(b - \max(a, b))\right].
\end{align}
Subtracting the maximum before doing the exponentiation prevents overflow.
Similarly, differences could be computed on the log scale. Assuming $x>y$,
we can use the following method:
\begin{align}
\log(x - y)  &= \log(\exp(\log x) - \exp(\log y)) \\
             &= \logdiffexp(\log x, \log y),
\end{align}
where $\logdiffexp$ is given by
\begin{align}
\logdiffexp(a, b) &= a
        + \log\left[1 - \exp(b-a)\right]
\end{align}
for $a > b$.

However,
logarithms only work for positive $x$, and our trending scores can be
negative (and zero) as well. In principle, any monotonic function will work
(and will allow the same total ordering of claims),
but we need one where arithmetic is possible without overflow.

Instead of $\log$, define a squashing function $S$ as:
\begin{align}
S(x) &=
    \left\{
        \begin{array}{lr}
            \log (1 + x), & \textnormal{if } x \geq 0, \\
            -\log(1 - x), & \textnormal{otherwise}.
        \end{array}
    \right.
\end{align}
The inverse, unsquashing function, is
\begin{align}
U(x) = S^{-1}(x) &=
    \left\{
        \begin{array}{lr}
            \exp(x) - 1,  & \textnormal{if } x \geq 0, \\
            1 - \exp(-x), & \textnormal{otherwise}.
        \end{array}
    \right.
\end{align}
For large positive values, these are approximately equal to log and exp
respectively, but for smaller values (and negative values), it remains
continuous --- see Figure~{fig:squash}.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.7\textwidth]{figures/squash.pdf}
\caption{The squashing function $S(x)$ is the solid black curve.
The dashed blue and pink curves are $\log(x)$ and $-\log(-x)$ respectively.
\label{fig:squash}}
\end{figure}

To compute updated trending scores, we need to add and multiply
values using Equations~\ref{eqn:sparse_update}~and~\ref{eqn:spike_mass}, but on the
squashed grade. In other words, if we have squashed values $x$ and
$y$, we will need the squashed sum
\begin{align}
x \oplus y &= S(U(x) + U(y))
\end{align}
and the squashed product
\begin{align}
x \otimes y &= S(U(x)U(y)).
\end{align}
These can be worked out in terms of logsumexp and logdiffexp, and thus can
be computed relatively easily without overflow.
The results are given in the Appendix.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.7\textwidth]{figures/response.pdf}
\caption{An example of how the trending score might vary over time
for a single claim.\label{fig:response}}
\end{figure}


In the actual code, we store squashed trending values, i.e., $S(Z)$ in the
notation of this paper, for every claim. When claims are touched, the
squashed trending values are updated. At block height 1,000,000, which occurred
in 2021, unsquashed trending values would have been of order
\begin{align}
\exp\left(\frac{10^{6}}{576}\right) &\approx 10^{753}.
\end{align}
However, the squashed values are only of order $10^6 / 576 \approx 1736$.
Differences in squashed trending values, which determine the ordering of claims
in applications, occur in the third, fourth, and subsequent digits.
With double precision storage, the situation will remain acceptable for
centuries. For example, at block height 100 million, expected around 2496 AD,
squashed trending scores will be around 150,000, with plenty of remaining
significant digits to allow for sorting.




\section{Acknowledgements}
BJB would like to thank Kevin Knuth (SUNY Albany) for inspiration.

\bibliographystyle{plainnat}
\bibliography{references.bib/references}

\appendix
\section{Squashed Arithmetic}
The squashed addition operator is broken down into four cases, depending
on the sign of the arguments:
\begin{align}
x \oplus y &= \left\{
    \begin{array}{lr}
   -\logsumexp(-x, \logdiffexp(-y, 0)),& x < 0~\textnormal{and}~y < 0, \\
    \logsumexp(x, \logdiffexp(y, 0)),& x \geq 0~\textnormal{and}~y \geq 0 \\
    \logsumexp(0, \logdiffexp(x, -y)), & \sign(x) \neq \sign(y)~\textnormal{and}~|x| \geq |y|\\
    -\logsumexp(0, \logdiffexp(-y, x)), & \sign(x) \neq \sign(y)~\textnormal{and}~|x| < |y|
    \end{array}
\right. .
\end{align}

Squashed multiplication is simpler:
\begin{align}
x \otimes y &= \sign(xy)
                \logsumexp\left(
                    \logdiffexp(|x|, 0) + \logdiffexp(|y|, 0),
                0\right).
\end{align}

\end{document}

