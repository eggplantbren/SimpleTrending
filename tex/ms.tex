\documentclass[a4paper, 12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage[left=1.5cm, right=1.5cm, bottom=2cm, top=2cm]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{natbib}
\newcommand{\logsumexp}{\textnormal{logsumexp}}

\title{The LBRY Trending Algorithm}
\author{Brendon J. Brewer}
\date{}

\begin{document}
\maketitle

\abstract{I describe the most recent update to the LBRY trending
algorithm. The ideas are similar to previous implementations, but with a
few new features.}

% Need this after the abstract
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

\section{Introduction}
LBRY needs a trending algorithm to rank claims by `recent popularity',
preferably using on-chain information such as changes in support. An
intuitive idea is to have increases and decreases in overall support cause
a `spike' in the trending score, which then fades away over time
(assuming no further changes occur). This idea has formed the basis of all the
implementations that I have written.

All claims, be they streams/files, channels, reposts, or collections, have
an associated trending score. Hubs (formerly wallet servers) compute and
save the trending score of every claim along with the other data. Several
views of content on LBRY Desktop and Odysee involve sorting claims by their
trending score, in descending order.

In 2021, Jack Robison rewrote a lot of the hub code to improve performance
and to ensure hub data integrity when a chain reorganisation occurs. The
backend now uses leveldb to store transaction and claim information, and
Elasticsearch to store claim information in a suitable form for clients to
query. Leveldb will be replaced with rocksdb soon.

\section{Exponentially Decaying Spikes}
Suppose that, at time (block height) $t$, the total amount of LBC staked
towards a claim is $\ell_t$, and its trending score is $Y_t$. At height
$(t+1)$, if the total amount of LBC remains the same, the trending score
is decayed towards zero:
\begin{align}
Y_{t+1} &= kY(t), \label{eqn:decay}
\end{align}
where $k$ is a decay constant which we set equal to
$\exp(-1/576) \approx 0.9982653$, so that the e-folding timescale is 576
blocks, approximately 24 hours.

If an LBC change occurs, the update is instead
\begin{align}
Y_{t+1} &= kY(t) + M(\ell_t, \ell_{t+1}), \label{eqn:decay_with_spike}
\end{align}
where the second term is the `mass' of a `trending spike', and depends on
the old and new LBC values. A simple idea is to set the mass of the trending
spike equal to the change in LBC:
\begin{align}
M(\ell_t, \ell_{t+1}) &= \ell_{t+1} - \ell_t.
\end{align}
However, in practice this simple choice might lead to undesirable results.
Many supports on the LBRY network are created by publishers themselves,
attempting to boost their own content's visibility. If an item receives
many organic views and a moderate number of small tips, one might expect it
to trend, yet with the linear choice for $M$, it will have no hope of
competing with a `whale' who can deposit 100,000 LBC in support without
blinking an eye. In Section~\ref{sec:spike_mass}, we simulate several
different patterns of support, and choose a function $M$ such that
the size of trending spikes makes `minnows' able to compete with `whales',
at least to a greater extent than they can with the linear choice.

\section{Nonlinearity --- the Spike Mass Function}\label{sec:spike_mass}
As briefly discussed in the last section, a linear choice of spike mass
function might not be ideal. When we choose this function, we are making
a decision about how significant one amount of LBC at one time is, compared
to a different amount of LBC at a different time. For example, perhaps
1000 LBC right now should be considered as
significant as 10,000 LBC 12 hours ago.

Our final choice of spike mass function is complex. I will describe simpler
versions at first, which will help in motivating the final, complicated, choice.
First, let's look at a choice where the LBC amounts are `softened' by raising
them to some power less than one (here I have chosen a power of 1/3, in
previous versions of the algorithm I used 1/4):
\begin{align}
M(\ell_t, \ell_{t+1}) &= \ell_{t+1}^{1/3} - \ell_t^{1/3}.
\end{align}
The size of a trending spike here would be the change in the `softened'
LBC amount. A change from 0 to 1 LBC would give a spike mass of 1,
as would a change from 1 to 1000 LBC, or 1000 to 1,000,000. It takes more
LBC to move a mountain than to move a molehill.

With this choice, it wouldn't matter whether a change took place in one large
step or in smaller increments. Specifically,
\begin{align}
M(x, y) + M(y, z)
    &= y^{1/3} - x^{1/3} + z^{1/3} - y^{1/3} \\
    &= z^{1/3} - x^{1/3} \\
    &= M(x, z).
\end{align}
This may not be entirely desirable. Genuinely popular claims on LBRY tend
to receive several smaller supports, rather than one large one, and intuitively
this ought to help. A trending algorithm should show what's trending,
as well as what's recently been boosted.




\section{Sparsity}
For most claims, most of the time, nothing happens. However, computing the
exponential decay using Equation~\ref{eqn:decay} would still incur a cost.
Right now, there are about 15 million active claims on the LBRY network,
and the intention is that there might be orders of magnitude more in the
future. We want to avoid decaying the trending score of 15 million claims
when nothing has happened to most of them. In the most recent implementation,
we have found a way to avoid decaying any claims, ever --- only touched claims
(those whose LBC values changed) ever need any computation.

\section{The `Squashed' Grade}
Suppose $x$ is a huge number, where double precision overflow might occur if
we were to use it in a calculation (this happens at around $10^{308}$).
If $x$ is positive, we could instead deal with logarithms. Then, to add
two huge numbers without overflow, we could use
\begin{align}
\log(x + y)  &= \log(\exp(\log x) + \exp(\log y)) \\
             &= \logsumexp(\log x, \log y),
\end{align}
where $\logsumexp(a, b) = $.

However,
logarithms only work for positive $x$, and our trending scores can be
negative (and zero) as well. In principle, any monotonic function will work
(and will allow the same total ordering of claims),
but we need one where arithmetic is possible without overflow.

Instead of $\log$, define a squashing function $S$ as:
\begin{align}
S(x) &=
    \left\{
        \begin{array}{lr}
            \log (1 + x), & \textnormal{if } x \geq 0, \\
            -\log(1 - x), & \textnormal{otherwise}.
        \end{array}
    \right.
\end{align}
The inverse, unsquashing function, is
\begin{align}
U(x) = S^{-1}(x) &=
    \left\{
        \begin{array}{lr}
            \exp(x) - 1,  & \textnormal{if } x \geq 0, \\
            1 - \exp(-x), & \textnormal{otherwise}.
        \end{array}
    \right.
\end{align}

To compute updated trending scores, we need to add values, but on the
squashed grade. In other words, if we have squashed values $S(x)$ and
$S(y)$, we will need the squashed sum
\begin{align}
S(U(x) + U(y)).
\end{align}
These can be worked out in terms of logsumexp and logdiffexp.


\end{document}

